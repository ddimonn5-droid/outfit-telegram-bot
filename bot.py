# bot.py
import os
import re
import html
import logging
import asyncio
import httpx
import requests
from typing import List, Optional, Tuple
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters

# ========= CONFIG =========
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
XAI_API_KEY = os.getenv("XAI_API_KEY")
PORT = int(os.getenv("PORT", 8443))
APP_URL = os.getenv("RENDER_EXTERNAL_URL")  # https://your-service.onrender.com

if not TELEGRAM_BOT_TOKEN or not XAI_API_KEY or not APP_URL:
    raise RuntimeError("Set TELEGRAM_BOT_TOKEN, XAI_API_KEY, RENDER_EXTERNAL_URL")

# ========= LOGGING =========
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("grok-bot")

# ========= CONSTANTS / HELPERS =========
URL_RE = re.compile(r"https?://[^\s\)\]\>]+")
IMG_EXT_RE = re.compile(r"\.(?:jpg|jpeg|png|gif|webp|bmp)(?:$|\?)", re.I)
EMOJIS = ["üëï", "üëñ", "üëü", "üß•", "üéí"]
HTTP_TIMEOUT = 10
CONCURRENCY = 16

# –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã (—á–µ–º —à–∏—Ä–µ ‚Äî —Ç–µ–º –ø—Ä–æ—â–µ –Ω–∞–π—Ç–∏ —Ç–æ–≤–∞—Ä—ã)
ALLOWED_WEBSITES = [
    "zara.com", "hm.com", "bershka.com", "asos.com", "zalando.",
    "lyst.com", "grailed.com", "nike.com", "adidas.com", "uniqlo.com",
    "levi.com", "converse.com"
]

# –†–µ–≥–µ–∫—Å—ã –¥–ª—è product pages –ø–æ –¥–æ–º–µ–Ω–∞–º
PRODUCT_PATTERNS = {
    # Zara: /en/us/...-p012345.html
    "zara.com":      r"/[a-z]{2}/[a-z]{2}/.+-p\d{5,}\.html",
    # H&M: /productpage.123456.html
    "hm.com":        r"/productpage\.\d+\.html",
    # Bershka: /ru/ru/...-c123456/p/123456789.html
    "bershka.com":   r"/[a-z]{2}/[a-z]{2}/[a-z-]+-c\d+/p/\d+\.html",
    # ASOS: /prd/12345678 –∏–ª–∏ /p/<slug>/12345678
    "asos.com":      r"/(prd/\d+|/p/[a-z0-9-]+/\d+)",
    # Zalando: /.../article/<CODE> –∏–ª–∏ /p/<CODE>
    "zalando.":      r"/.*(article|p)/[A-Z0-9]{6,}",
    # Lyst: –æ–±—ã—á–Ω–æ /clothing|shoes|accessories/...<digits>/
    "lyst.com":      r"/(clothing|shoes|accessories)/.+\d{4,}/?",
    # Grailed: /listings/12345678
    "grailed.com":   r"/listings/\d+",
    # Nike product: /t/<slug>-<code>  –∏–ª–∏ /launch/t/<slug>
    "nike.com":      r"/(launch/)?t/[a-z0-9-]+",
    # Adidas: /en/us/.../<CODE>.html
    "adidas.com":    r"/[a-z]{2}/[a-z]{2}/.+/[A-Z0-9]{6,}\.html",
    # UNIQLO: /products/<slug>  –∏–ª–∏ /product/<slug>
    "uniqlo.com":    r"/products?/[a-z0-9-]+",
    # Levi's: /p/<CODE>  –∏–ª–∏ /product/<slug>
    "levi.com":      r"/(p|product)/[A-Za-z0-9\-]{5,}",
    # Converse: /shop/p/<slug>  –∏–ª–∏ /p/<slug>
    "converse.com":  r"/(shop/)?p/[a-z0-9-]+",
}

def domain_of(url: str) -> str:
    try:
        return requests.utils.urlparse(url).netloc.lower()
    except Exception:
        return ""

def clean_url(u: str) -> str:
    u = html.unescape(u).replace("\u200b", "").replace("\u200e", "").replace("\u200f", "")
    u = u.strip()
    while u and u[-1] in ".,;:)]'\"":
        u = u[:-1]
    return u

def extract_urls(text: str) -> List[str]:
    return [clean_url(u) for u in URL_RE.findall(text or "")]

def looks_like_product(url: str) -> bool:
    if IMG_EXT_RE.search(url):
        return False
    host = domain_of(url)
    path = requests.utils.urlparse(url).path
    if not host or not path or url.endswith("/"):
        return False
    # —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    if not any(allow in host for allow in ALLOWED_WEBSITES):
        return False
    # –¥–æ–º–µ–Ω–Ω—ã–µ —Ä–µ–≥–µ–∫—Å—ã
    for key, rx in PRODUCT_PATTERNS.items():
        if key in host:
            return re.search(rx, path, flags=re.I) is not None
    return False

# ========= HTTP utils =========
async def http_ok_html(url: str) -> bool:
    if IMG_EXT_RE.search(url):
        return False
    try:
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            r = await client.get(url)
            if r.status_code == 200:
                ctype = r.headers.get("content-type", "")
                return "text/html" in ctype
    except Exception as e:
        logger.debug("validate fail %s -> %s", url, e)
    return False

async def fetch_title(url: str) -> Optional[str]:
    try:
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            r = await client.get(url)
            if r.status_code != 200:
                return None
            m = re.search(r"<title[^>]*>(.*?)</title>", r.text, flags=re.I | re.S)
            if not m:
                return None
            title = html.unescape(m.group(1)).strip()
            title = re.sub(r"\s+", " ", title)
            return title[:120]
    except Exception:
        return None

async def validate_and_title_batch(urls: List[str], need: int) -> List[Tuple[str, Optional[str]]]:
    out, seen = [], set()
    sem = asyncio.Semaphore(CONCURRENCY)

    async def worker(u):
        async with sem:
            if not looks_like_product(u):
                return None
            ok = await http_ok_html(u)
            if not ok:
                return None
            title = await fetch_title(u)
            return (u, title)

    tasks = [asyncio.create_task(worker(u)) for u in urls]
    for t in asyncio.as_completed(tasks):
        try:
            res = await t
        except Exception:
            res = None
        if res:
            u, ti = res
            if u not in seen:
                seen.add(u)
                out.append((u, ti))
                if len(out) >= need:
                    break
    return out

# ========= Grok =========
def build_system_prompt(strict: bool = False) -> str:
    base = (
        "–¢—ã –º–æ–¥–Ω—ã–π —Å—Ç–∏–ª–∏—Å—Ç. –ü–æ–¥–±–∏—Ä–∞–π —Å—Ç—Ä–æ–≥–æ 5 –≤–µ—â–µ–π: üëï —Ñ—É—Ç–±–æ–ª–∫–∞, üëñ –¥–∂–∏–Ω—Å—ã, üëü –∫—Ä–æ—Å—Å–æ–≤–∫–∏, üß• –∫—É—Ä—Ç–∫–∞, üéí –∞–∫—Å–µ—Å—Å—É–∞—Ä.\n"
        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: Emoji –ù–∞–∑–≤–∞–Ω–∏–µ ‚Äî —Å—Å—ã–ª–∫–∞ (–æ–¥–Ω–∞ –≤–µ—â—å –Ω–∞ —Å—Ç—Ä–æ–∫—É).\n\n"
        "‚ÄºÔ∏è –¢–û–õ–¨–ö–û –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ (product pages). –ó–∞–ø—Ä–µ—â–µ–Ω—ã: —Å—Ç–∞—Ç—å–∏, –æ–±–∑–æ—Ä—ã, –±–ª–æ–≥–∏, –Ω–æ–≤–æ—Å—Ç–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.\n"
        "–†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –º–∞–≥–∞–∑–∏–Ω—ã (—Ç–æ–ª—å–∫–æ —ç—Ç–∏ –¥–æ–º–µ–Ω—ã):\n"
        " - Zara: zara.com\n"
        " - H&M: hm.com\n"
        " - Bershka: bershka.com\n"
        " - ASOS: asos.com\n"
        " - Zalando: –ª—é–±—ã–µ –ø–æ–¥–¥–æ–º–µ–Ω—ã (zalando.*)\n"
        " - Lyst: lyst.com (—Ç–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤)\n"
        " - Grailed: grailed.com (—Ç–æ–ª—å–∫–æ listings)\n"
        " - Nike: nike.com\n"
        " - Adidas: adidas.com\n"
        " - UNIQLO: uniqlo.com\n"
        " - Levi's: levi.com\n"
        " - Converse: converse.com\n"
        "–°—Å—ã–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–≤–∞—Ä–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–ª–∏ —Å–ª–∞–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'productpage.123456', '-p012345', 'id=12345', '/dp/...').\n"
    )
    if strict:
        base += "–°–¢–†–û–ì–û: –≤–µ—Ä–Ω–∏ –†–û–í–ù–û 5 URL –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤, –ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ, –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π."
    return base

def ask_grok(user_text: str, strict: bool = False, max_search_results: int = 20) -> dict:
    payload = {
        "model": "grok-4",
        "messages": [
            {"role": "system", "content": build_system_prompt(strict=strict)},
            {"role": "user", "content": f"–ü–æ–¥–±–µ—Ä–∏ –∞—É—Ç—Ñ–∏—Ç (—Ñ—É—Ç–±–æ–ª–∫–∞, –¥–∂–∏–Ω—Å—ã, –∫—Ä–æ—Å—Å–æ–≤–∫–∏, –∫—É—Ä—Ç–∫–∞, –∞–∫—Å–µ—Å—Å—É–∞—Ä): {user_text}"}
        ],
        "max_tokens": 600,
        "search_parameters": {
            "mode": "on",
            "return_citations": True,
            "max_search_results": max_search_results,
            "sources": [{
                "type": "web",
                "allowed_websites": ALLOWED_WEBSITES
            }]
        },
        "temperature": 0.12
    }
    logger.info("Grok request strict=%s", strict)
    url = "https://api.x.ai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {XAI_API_KEY}", "Content-Type": "application/json"}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    r.raise_for_status()
    return r.json()

# ========= Bot Handlers =========
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kb = [[InlineKeyboardButton("–ü—Ä–∏–º–µ—Ä: –∫—ç–∂—É–∞–ª", callback_data="casual")],
          [InlineKeyboardButton("–ü–æ–º–æ—â—å", callback_data="help")]]
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç! –ù–∞–ø–∏—à–∏ —Å—Ç–∏–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´—É–ª–∏—á–Ω—ã–π —Å–ø–æ—Ä—Ç¬ª, ¬´–æ—Ñ–∏—Å –ª–µ—Ç–æ–º¬ª, ¬´–≤–µ—á–µ—Ä–∏–Ω–∫–∞ 90-—Ö¬ª), "
        "–∏ —è –ø–æ–¥–±–µ—Ä—É 5 —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.",
        reply_markup=InlineKeyboardMarkup(kb)
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–Ø –≤–æ–∑–≤—Ä–∞—â–∞—é —Ä–æ–≤–Ω–æ 5 product pages (—Ñ—É—Ç–±–æ–ª–∫–∞, –¥–∂–∏–Ω—Å—ã, –∫—Ä–æ—Å—Å–æ–≤–∫–∏, –∫—É—Ä—Ç–∫–∞, –∞–∫—Å–µ—Å—Å—É–∞—Ä) "
        "–∏–∑ –º–∞–≥–∞–∑–∏–Ω–æ–≤: Zara, H&M, Bershka, ASOS, Zalando, Lyst, Grailed, Nike, Adidas, UNIQLO, Levi's, Converse."
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = (update.message.text or "").strip()
    if not user_text:
        await update.message.reply_text("–ù–∞–ø–∏—à–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —á—Ç–æ –∏—Å–∫–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'casual –Ω–∞ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å').")
        return

    await update.message.reply_text("üîé –ò—â—É —Ç–æ–≤–∞—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")

    found: List[Tuple[str, Optional[str]]] = []
    attempts = 0
    tried_urls: List[str] = []

    # –î–æ 5 –ø–æ–ø—ã—Ç–æ–∫: 1 –æ–±—ã—á–Ω–∞—è + 4 —Å—Ç—Ä–æ–≥–∏—Ö (—Ç–æ–ª—å–∫–æ URL)
    while len(found) < 5 and attempts < 5:
        strict = attempts >= 1
        try:
            raw = ask_grok(user_text, strict=strict, max_search_results=25 if strict else 15)
        except Exception:
            logger.exception("Grok error")
            await asyncio.sleep(0.6)
            attempts += 1
            continue

        citations = raw.get("citations") or []
        choice = (raw.get("choices") or [{}])[0]
        msg = choice.get("message") or {}
        text = html.unescape(msg.get("content") or "")
        from_text = extract_urls(text)

        candidates = []
        for u in citations + from_text:
            cu = clean_url(u)
            if cu and cu not in candidates:
                candidates.append(cu)

        logger.info("Attempt %s: %d candidates (cit=%d, text=%d)",
                    attempts, len(candidates), len(citations), len(from_text))
        logger.info("Sample: %s", candidates[:8])

        to_check = candidates
        validated = await validate_and_title_batch(to_check, need=5 - len(found))
        for (u, title) in validated:
            if all(u != x[0] for x in found):
                found.append((u, title))
                if len(found) >= 5:
                    break

        tried_urls.extend(candidates)
        attempts += 1
        if len(found) < 5:
            await asyncio.sleep(0.6)

    # –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –º–µ–Ω—å—à–µ 5 ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –≤—Å—ë, —á—Ç–æ —É–≤–∏–¥–µ–ª–∏ (–≤–¥—Ä—É–≥ —á–∞—Å—Ç—å –ø—Ä–æ—à–ª–∞ —Ç–µ–ø–µ—Ä—å)
    if len(found) < 5 and tried_urls:
        more = await validate_and_title_batch(
            [u for u in tried_urls if all(u != x[0] for x in found)],
            need=5 - len(found)
        )
        for (u, title) in more:
            if all(u != x[0] for x in found):
                found.append((u, title))
                if len(found) >= 5:
                    break

    if not found:
        await update.message.reply_text(
            "üòî –ü–æ—Ö–æ–∂–µ, –º–∞–≥–∞–∑–∏–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–∏–ª–∏ –¥–æ—Å—Ç—É–ø. –£—Ç–æ—á–Ω–∏ –±—Ä–µ–Ω–¥/—Ç–∏–ø –≤–µ—â–∏ (–Ω–∞–ø—Ä. 'Zara –±–µ–ª–∞—è —Ñ—É—Ç–±–æ–ª–∫–∞')."
        )
        return

    # –≤—ã–≤–æ–¥–∏–º: —Å—Ç—Ä–æ–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º + –∫–Ω–æ–ø–∫–∞ —Å —Å—Å—ã–ª–∫–æ–π
    lines = []
    buttons = []
    for i, (url, title) in enumerate(found[:5]):
        emoji = EMOJIS[i] if i < len(EMOJIS) else "üõçÔ∏è"
        label = (title or "–¢–æ–≤–∞—Ä").strip()
        if len(label) > 80:
            label = label[:77] + "..."
        lines.append(f"{emoji} {label}\n{url}")
        buttons.append([InlineKeyboardButton(f"{emoji} –û—Ç–∫—Ä—ã—Ç—å", url=url)])

    text = "–í–æ—Ç —Ç–≤–æ–π –∞—É—Ç—Ñ–∏—Ç (—Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã):\n\n" + "\n\n".join(lines)
    await update.message.reply_text(text, reply_markup=InlineKeyboardMarkup(buttons), disable_web_page_preview=True)

# ========= MAIN =========
def main():
    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    logger.info("Bot starting webhook...")
    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=TELEGRAM_BOT_TOKEN,
        webhook_url=f"{APP_URL}/{TELEGRAM_BOT_TOKEN}"
    )

if __name__ == "__main__":
    main()
