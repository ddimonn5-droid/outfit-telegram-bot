# bot.py
import os
import re
import html
import logging
import asyncio
import httpx
import requests
from typing import List, Optional, Tuple, Dict
from urllib.parse import quote_plus
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters

# ========= CONFIG =========
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
XAI_API_KEY = os.getenv("XAI_API_KEY")
PORT = int(os.getenv("PORT", 8443))
APP_URL = os.getenv("RENDER_EXTERNAL_URL")

if not TELEGRAM_BOT_TOKEN or not XAI_API_KEY or not APP_URL:
    raise RuntimeError("Set TELEGRAM_BOT_TOKEN, XAI_API_KEY, RENDER_EXTERNAL_URL env vars")

# ========= LOGGING =========
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("grok-bot")

# ========= CONSTANTS / HELPERS =========
URL_RE = re.compile(r"https?://[^\s\)\]\>]+")
IMG_EXT_RE = re.compile(r"\.(?:jpg|jpeg|png|gif|webp|bmp)(?:$|\?)", re.I)
EMOJIS = ["üëï", "üëñ", "üëü", "üß•", "üéí"]
HTTP_TIMEOUT = 10
CONCURRENCY = 16

# –î–æ–º–µ–Ω–Ω—ã–µ —Ä–µ–≥–µ–∫—Å—ã product pages
PRODUCT_PATTERNS = {
    "zara.com":      r"/[a-z]{2}/[a-z]{2}/.+-p\d{5,}\.html",
    "hm.com":        r"/productpage\.\d+\.html",
    "bershka.com":   r"/[a-z]{2}/[a-z]{2}/[a-z-]+-c\d+/p/\d+\.html",
    "asos.com":      r"/(prd/\d+|/p/[a-z0-9-]+/\d+)",
    "zalando.":      r"/.*(article|p)/[A-Z0-9]{6,}",
    "lyst.com":      r"/(clothing|shoes|accessories)/.+\d{4,}/?",
    "grailed.com":   r"/listings/\d+",
    "nike.com":      r"/(launch/)?t/[a-z0-9-]+",
    "adidas.com":    r"/[a-z]{2}/[a-z]{2}/.+/[A-Z0-9]{6,}\.html",
    "uniqlo.com":    r"/products?/[a-z0-9-]+",
    "levi.com":      r"/(p|product)/[A-Za-z0-9\-]{5,}",
    "converse.com":  r"/(shop/)?p/[a-z0-9-]+",
}

SEARCH_ENDPOINTS: Dict[str, str] = {
    # –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ URL; –º—ã –ø–æ—Ç–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ product-—Å—Å—ã–ª–∫–∏ —Ä–µ–≥–µ–∫—Å–∞–º–∏ —Å–≤–µ—Ä—Ö—É
    "zara.com":     "https://www.zara.com/ww/en/search?searchTerm={q}",
    "hm.com":       "https://www2.hm.com/en_us/search-results.html?q={q}",
    "bershka.com":  "https://www.bershka.com/ww/search?q={q}",
    "asos.com":     "https://www.asos.com/search/?q={q}",
    "zalando.com":  "https://www.zalando.com/catalog/?q={q}",
    "nike.com":     "https://www.nike.com/w?q={q}",
    "adidas.com":   "https://www.adidas.com/us/search?q={q}",
    "uniqlo.com":   "https://www.uniqlo.com/us/en/search/?q={q}",
    "levi.com":     "https://www.levi.com/US/en_US/search/{q}",
    "converse.com": "https://www.converse.com/search?q={q}",
    # –∞–≥–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ –ø–æ—Ç–æ–º, –Ω–æ —Ä–µ–≥–µ–∫—Å—ã —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞ lyst/grailed
    "lyst.com":     "https://www.lyst.com/search/?q={q}",
    "grailed.com":  "https://www.grailed.com/shop?q={q}",
}

# –ö–∞—Ä—Ç–∞ —Å–ª–æ—Ç–æ–≤ -> –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
ITEM_QUERIES = [
    ("üëï", "—Ñ—É—Ç–±–æ–ª–∫–∞", ["t shirt", "tee", "tshirt"]),
    ("üëñ", "–¥–∂–∏–Ω—Å—ã",   ["jeans", "denim jeans"]),
    ("üëü", "–∫—Ä–æ—Å—Å–æ–≤–∫–∏",["sneakers", "trainers"]),
    ("üß•", "–∫—É—Ä—Ç–∫–∞",   ["jacket"]),
    ("üéí", "–∞–∫—Å–µ—Å—Å—É–∞—Ä",["backpack", "belt", "cap"]),
]

def domain_of(url: str) -> str:
    try:
        return requests.utils.urlparse(url).netloc.lower()
    except Exception:
        return ""

def clean_url(u: str) -> str:
    u = html.unescape(u).replace("\u200b", "").replace("\u200e", "").replace("\u200f", "")
    u = u.strip()
    while u and u[-1] in ".,;:)]'\"":
        u = u[:-1]
    return u

def extract_urls(text: str) -> List[str]:
    return [clean_url(u) for u in URL_RE.findall(text or "")]

def looks_like_product(url: str) -> bool:
    if IMG_EXT_RE.search(url):
        return False
    host = domain_of(url)
    path = requests.utils.urlparse(url).path
    if not host or not path or url.endswith("/"):
        return False
    for key, rx in PRODUCT_PATTERNS.items():
        if key in host:
            return re.search(rx, path, flags=re.I) is not None
    return False

# ========= HTTP utils =========
async def http_ok_html(url: str) -> bool:
    if IMG_EXT_RE.search(url):
        return False
    try:
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            r = await client.get(url)
            return r.status_code == 200 and "text/html" in (r.headers.get("content-type", ""))
    except Exception as e:
        logger.debug("validate fail %s -> %s", url, e)
        return False

async def fetch_title(url: str) -> Optional[str]:
    try:
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            r = await client.get(url)
            if r.status_code != 200:
                return None
            m = re.search(r"<title[^>]*>(.*?)</title>", r.text, flags=re.I | re.S)
            if not m:
                return None
            title = html.unescape(m.group(1)).strip()
            title = re.sub(r"\s+", " ", title)
            return title[:120]
    except Exception:
        return None

async def validate_and_title_batch(urls: List[str], need: int) -> List[Tuple[str, Optional[str]]]:
    out, seen = [], set()
    sem = asyncio.Semaphore(CONCURRENCY)

    async def worker(u):
        async with sem:
            if not looks_like_product(u):
                return None
            ok = await http_ok_html(u)
            if not ok:
                return None
            title = await fetch_title(u)
            return (u, title)

    tasks = [asyncio.create_task(worker(u)) for u in urls]
    for t in asyncio.as_completed(tasks):
        try:
            res = await t
        except Exception:
            res = None
        if res:
            u, ti = res
            if u not in seen:
                seen.add(u)
                out.append((u, ti))
                if len(out) >= need:
                    break
    return out

# ========= Grok =========
def build_system_prompt(strict: bool = False) -> str:
    base = (
        "–¢—ã –º–æ–¥–Ω—ã–π —Å—Ç–∏–ª–∏—Å—Ç. –ü–æ–¥–±–∏—Ä–∞–π —Å—Ç—Ä–æ–≥–æ 5 –≤–µ—â–µ–π: üëï —Ñ—É—Ç–±–æ–ª–∫–∞, üëñ –¥–∂–∏–Ω—Å—ã, üëü –∫—Ä–æ—Å—Å–æ–≤–∫–∏, üß• –∫—É—Ä—Ç–∫–∞, üéí –∞–∫—Å–µ—Å—Å—É–∞—Ä.\n"
        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: Emoji –ù–∞–∑–≤–∞–Ω–∏–µ ‚Äî —Å—Å—ã–ª–∫–∞ (–æ–¥–Ω–∞ –≤–µ—â—å –Ω–∞ —Å—Ç—Ä–æ–∫—É).\n\n"
        "‚ÄºÔ∏è –¢–û–õ–¨–ö–û –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ (product pages). –ó–∞–ø—Ä–µ—â–µ–Ω—ã: —Å—Ç–∞—Ç—å–∏, –æ–±–∑–æ—Ä—ã, –±–ª–æ–≥–∏, –Ω–æ–≤–æ—Å—Ç–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –≥–ª–∞–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.\n"
        "–î–æ–ø—É—Å–∫–∞—é—Ç—Å—è –¥–æ–º–µ–Ω—ã –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ (Zara, H&M, Bershka, ASOS, Zalando, Lyst, Grailed, Nike, Adidas, UNIQLO, Levi's, Converse).\n"
        "–°—Å—ã–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–≤–∞—Ä–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–ª–∏ —Å–ª–∞–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'productpage.123456', '-p012345', 'id=12345', '/dp/...').\n"
    )
    if strict:
        base += "–°–¢–†–û–ì–û: –≤–µ—Ä–Ω–∏ –†–û–í–ù–û 5 URL –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤, –ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ, –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏–π."
    return base

def grok_call(payload: dict) -> dict:
    url = "https://api.x.ai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {XAI_API_KEY}", "Content-Type": "application/json"}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code >= 400:
        try:
            logger.error("Grok 4xx/5xx: %s\nResponse: %s", r.status_code, r.text[:2000])
        except Exception:
            logger.error("Grok 4xx/5xx: %s (cannot decode body)", r.status_code)
        r.raise_for_status()
    return r.json()

def ask_grok(user_text: str, strict: bool = False, max_search_results: int = 20) -> dict:
    payload = {
        "model": "grok-4",
        "messages": [
            {"role": "system", "content": build_system_prompt(strict=strict)},
            {"role": "user", "content": f"–ü–æ–¥–±–µ—Ä–∏ –∞—É—Ç—Ñ–∏—Ç (—Ñ—É—Ç–±–æ–ª–∫–∞, –¥–∂–∏–Ω—Å—ã, –∫—Ä–æ—Å—Å–æ–≤–∫–∏, –∫—É—Ä—Ç–∫–∞, –∞–∫—Å–µ—Å—Å—É–∞—Ä): {user_text}"}
        ],
        "max_tokens": 600,
        "search_parameters": {
            "mode": "on",
            "return_citations": True,
            "max_search_results": max_search_results,
            "sources": [{"type": "web"}]  # –±–µ–∑ allowed_websites, –∏–Ω–∞—á–µ 400
        },
        "temperature": 0.12
    }
    logger.info("Grok request strict=%s", strict)
    return grok_call(payload)

# ========= Fallback Site Search (–±–µ–∑ Grok) =========
async def site_search_first_product(site: str, query: str) -> Optional[str]:
    """–û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞–≥–∞–∑–∏–Ω–∞ –∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –ø–µ—Ä–≤—É—é product-—Å—Å—ã–ª–∫—É –ø–æ –Ω–∞—à–µ–º—É –¥–æ–º–µ–Ω–Ω–æ–º—É —Ä–µ–≥–µ–∫—Å—É."""
    base = SEARCH_ENDPOINTS[site]
    url = base.format(q=quote_plus(query))
    try:
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT, follow_redirects=True) as client:
            r = await client.get(url)
            if r.status_code != 200:
                return None
            html_text = r.text
            # –¥–æ—Å—Ç–∞–Ω–µ–º –≤—Å–µ URL —ç—Ç–æ–≥–æ –¥–æ–º–µ–Ω–∞
            host = site
            # –æ–±—â–∏–π –∂–∞–¥–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ —ç—Ç–æ–≥–æ –¥–æ–º–µ–Ω–∞
            urls = re.findall(rf"https?://[^\s\"']*{re.escape(host)}/[^\s\"']+", html_text, flags=re.I)
            # –ø—Ä–æ–≥–æ–Ω –ø–æ looks_like_product
            for u in urls:
                cu = clean_url(u)
                if looks_like_product(cu):
                    ok = await http_ok_html(cu)
                    if ok:
                        return cu
    except Exception as e:
        logger.debug("site_search_first_product fail %s -> %s", site, e)
    return None

async def guaranteed_find_products(user_text: str) -> List[Tuple[str, Optional[str]]]:
    """
    –§–æ–ª–±—ç–∫, –∫–æ—Ç–æ—Ä—ã–π –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ 5 –∫–∞—Ä—Ç–æ—á–µ–∫:
    –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ (üëï, üëñ, üëü, üß•, üéí) –ø—Ä–æ–±–µ–≥–∞–µ–º –ø–æ –º–∞–≥–∞–∑–∏–Ω–∞–º –∏ –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π product-URL.
    """
    items = ITEM_QUERIES  # —Ñ–∏–∫—Å: 5 —Å–ª–æ—Ç–æ–≤
    sites_priority = [
        "zara.com", "hm.com", "bershka.com", "asos.com",
        "zalando.com", "nike.com", "adidas.com", "uniqlo.com",
        "levi.com", "converse.com", "lyst.com", "grailed.com"
    ]
    results: List[Tuple[str, Optional[str]]] = []

    for emoji, ru_name, queries in items:
        found_url: Optional[str] = None
        # —Å–∫–ª–µ–∏–º –∑–∞–ø—Ä–æ—Å: —á—Ç–æ –ø–æ–ø—Ä–æ—Å–∏–ª —é–∑–µ—Ä + –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        q_variants = [f"{user_text} {ru_name}"] + queries
        for q in q_variants:
            for site in sites_priority:
                url = await site_search_first_product(site, q)
                if url:
                    title = await fetch_title(url)
                    results.append((url, title))
                    found_url = url
                    break
            if found_url:
                break
        if not found_url:
            # –µ—Å–ª–∏ –¥–∞–∂–µ —Ç–∞–∫ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –æ—Å—Ç–∞–≤–∏–º —Å–ª–æ—Ç –ø—É—Å—Ç—ã–º, –ø–æ–∑–∂–µ –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ–±—Ä–∞—Ç—å –∏–∑ –æ–±—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫
            logger.warning("Fallback search: not found for slot %s (%s)", emoji, ru_name)

    # –µ—Å–ª–∏ –≥–¥–µ-—Ç–æ –ø—É—Å—Ç–æ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ–±—Ä–∞—Ç—å –∏–∑ –ª—é–±—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ –≤—Å–µ–º —Å–∞–π—Ç–∞–º –Ω–∞ –æ–±—â—É—é —Å—Ç—Ä–æ–∫—É –∑–∞–ø—Ä–æ—Å–∞
    if len(results) < 5:
        extra_needed = 5 - len(results)
        pool = []
        for site in sites_priority:
            u = await site_search_first_product(site, user_text)
            if u:
                pool.append(u)
        # –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –±–µ—Ä—ë–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ
        validated = await validate_and_title_batch(pool, need=extra_needed)
        results.extend(validated)

    return results[:5]

# ========= Bot Handlers =========
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    kb = [[InlineKeyboardButton("–ü—Ä–∏–º–µ—Ä: –∫—ç–∂—É–∞–ª", callback_data="casual")],
          [InlineKeyboardButton("–ü–æ–º–æ—â—å", callback_data="help")]]
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç! –ù–∞–ø–∏—à–∏ —Å—Ç–∏–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´—É–ª–∏—á–Ω—ã–π —Å–ø–æ—Ä—Ç¬ª, ¬´–æ—Ñ–∏—Å –ª–µ—Ç–æ–º¬ª, ¬´–≤–µ—á–µ—Ä–∏–Ω–∫–∞ 90-—Ö¬ª), "
        "–∏ —è –ø–æ–¥–±–µ—Ä—É 5 —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤.",
        reply_markup=InlineKeyboardMarkup(kb)
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–Ø –≤–µ—Ä–Ω—É —Ä–æ–≤–Ω–æ 5 product pages (—Ñ—É—Ç–±–æ–ª–∫–∞, –¥–∂–∏–Ω—Å—ã, –∫—Ä–æ—Å—Å–æ–≤–∫–∏, –∫—É—Ä—Ç–∫–∞, –∞–∫—Å–µ—Å—Å—É–∞—Ä) "
        "–∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤. –ú–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –±—Ä–µ–Ω–¥/–±—é–¥–∂–µ—Ç/–º–∞—Ç–µ—Ä–∏–∞–ª."
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = (update.message.text or "").strip()
    if not user_text:
        await update.message.reply_text("–ù–∞–ø–∏—à–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —á—Ç–æ –∏—Å–∫–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'casual –Ω–∞ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å').")
        return

    await update.message.reply_text("üîé –ò—â—É —Ç–æ–≤–∞—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")

    # 1) –ü—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ Grok (–Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫)
    found: List[Tuple[str, Optional[str]]] = []
    attempts = 0
    tried_urls: List[str] = []

    while len(found) < 5 and attempts < 5:
        strict = attempts >= 1
        try:
            raw = ask_grok(user_text, strict=strict, max_search_results=25 if strict else 15)
        except Exception:
            logger.exception("Grok error")
            await asyncio.sleep(0.5)
            attempts += 1
            continue

        citations = raw.get("citations") or []
        choice = (raw.get("choices") or [{}])[0]
        msg = choice.get("message") or {}
        text = html.unescape(msg.get("content") or "")
        from_text = extract_urls(text)

        candidates = []
        for u in citations + from_text:
            cu = clean_url(u)
            if cu and cu not in candidates:
                candidates.append(cu)

        logger.info("Attempt %s: %d candidates (cit=%d, text=%d)",
                    attempts, len(candidates), len(citations), len(from_text))
        logger.info("Sample: %s", candidates[:8])

        validated = await validate_and_title_batch(candidates, need=5 - len(found))
        for (u, title) in validated:
            if all(u != x[0] for x in found):
                found.append((u, title))
                if len(found) >= 5:
                    break

        tried_urls.extend(candidates)
        attempts += 1
        if len(found) < 5:
            await asyncio.sleep(0.4)

    # 2) –ï—Å–ª–∏ Grok –Ω–µ –¥–æ–±–∏–ª –¥–æ 5 ‚Äî –≤–∫–ª—é—á–∞–µ–º –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ñ—Ñ–ª–∞–π–Ω-—Ñ–æ–ª–±—ç–∫ –ø–æ —Å–∞–π—Ç–∞–º
    if len(found) < 5:
        need = 5 - len(found)
        logger.info("FALLBACK site search enabled (need %s)", need)
        more = await guaranteed_find_products(user_text)
        # –ø—Ä–∏–≤–∏–Ω—Ç–∏–º —Ç–æ, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–µ–π
        for (u, title) in more:
            if all(u != x[0] for x in found):
                found.append((u, title))
                if len(found) >= 5:
                    break

    # 3) –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –ø—É—Å—Ç–æ (–ø–æ—á—Ç–∏ –Ω–µ—Ä–µ–∞–ª—å–Ω–æ —Å —Ñ–æ–ª–±—ç–∫–æ–º) ‚Äî –º—è–≥–∫–æ —Å–æ–æ–±—â–∏–º
    if not found:
        await update.message.reply_text(
            "üòî –ù–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–∫—Ä—ã–≤–∞–µ–º—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤. –£—Ç–æ—á–Ω–∏ –±—Ä–µ–Ω–¥/–º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'Zara –±–µ–ª–∞—è —Ñ—É—Ç–±–æ–ª–∫–∞, slim fit')."
        )
        return

    # 4) –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥: —Å—Ç—Ä–æ–∫–∞ —Å title –∏ –∫–Ω–æ–ø–∫–∞-—Å—Å—ã–ª–∫–∞
    lines = []
    buttons = []
    for i, (url, title) in enumerate(found[:5]):
        emoji = EMOJIS[i] if i < len(EMOJIS) else "üõçÔ∏è"
        label = (title or "–¢–æ–≤–∞—Ä").strip()
        if len(label) > 80:
            label = label[:77] + "..."
        lines.append(f"{emoji} {label}\n{url}")
        buttons.append([InlineKeyboardButton(f"{emoji} –û—Ç–∫—Ä—ã—Ç—å", url=url)])

    text = "–í–æ—Ç —Ç–≤–æ–π –∞—É—Ç—Ñ–∏—Ç (—Ä–µ–∞–ª—å–Ω—ã–µ product pages):\n\n" + "\n\n".join(lines)
    await update.message.reply_text(text, reply_markup=InlineKeyboardMarkup(buttons), disable_web_page_preview=True)

# ========= MAIN =========
def main():
    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    logger.info("Bot starting webhook...")
    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=TELEGRAM_BOT_TOKEN,
        webhook_url=f"{APP_URL}/{TELEGRAM_BOT_TOKEN}"
    )

if __name__ == "__main__":
    main()
